- block:
  - name: package > intall dependencies
    package:
      name: "{{ item }}"
    with_items:
      - lvm2

  - name: lvm | lvg > create/remove volume group.s
    lvg:
      vg:    "{{ item.vgname }}"
      pvs:   "{{ item.pv    | join(',') }}"
      state: "{{ item.state | default('present') }}"
      force: "{{ true if (item.state is defined and item.state == 'absent') else false }}"
    with_items:
      - "{{ lvm }}"

  - name: lvm | lvol > create/remove logical volume.s
    lvol:
      vg:     "{{ item.0.vgname }}"
      pvs:    "{{ item.0.pv  | join(',') }}"
      lv:     "{{ item.1.name }}"
      size:   "{{ item.1.size }}"
      opts:   "{{ item.1.opts | default(omit) }}"
      shrink: false
      force:  "{{ true if (item.state is defined and item.state == 'absent') else false }}"
      state:  "{{ item.1.state | default('present') }}"
    when: item.0.state is not defined or item.0.state == "present"
    with_subelements:
      - "{{ lvm }}"
      - lv
      - skip_missing: true

  # A DELETE
  # - name: lvm | lvol > create RAM partition
  #   lvol:
  #     vg:     "{{ lvm.0.vgname }}"
  #     # pvs:    "{{ lvm.0.pv.0 }}"
  #     lv:     "{{ item.1.name }}"
  #     size:   "{{ item.1.size }}"
  #     opts:   "{{ item.1.opts | default(omit) }}"
  #     shrink: false
  #     force:  "{{ true if (item.state is defined and item.state == 'absent') else false }}"
  #     state:  "{{ item.1.state | default('present') }}"
  #   when: item.0.state is not defined or item.0.state == "present"

  ## fill the logical volume btrfs with a stream of zeros
  ## dd if=/dev/zero of=/dev/lvm/btrfs bs=1M status=progress oflag=direct

  - name: shell > convert data logical volume as thin-pool
    shell: >
      lvs lvm/pool | grep -I twi-a-tz ||
      lvconvert -y --type thin-pool lvm/pool
    when: lvm_pool

  # IF lvm_cache_enabled = true
  # create cache only if lvm/data is on a slower device than the cache
  # - name: shell > convert lvm cache_metadata volume as cache_metadata thinpool
  #   shell: lvconvert -y --type cache-pool --poolmetadata lvm/cache_metadata lvm/cache_data
  # - name: shell > convert lvm cache_data volume as cache_data thinpool
  #   shell: lvconvert -y --type cache --cachepool lvm/cache_data lvm/data

  when: lvm_enable
  tags: lvm

# - block:
#   ## list all existing lvm group THEN remove them
#   - name: lvm | lvg > remove older lvm groups
#     lvg:
#       vg:    "{{ item }}"
#       force: yes
#       state: absent
#     with_items:
#       - lvm
#       - os
#       - pool
#       - pve

#   # - name: "[parted] get {{ device }} information"
#   #   parted:
#   #     device: "{{ device }}"
#   #     unit: MiB                 # always needed when probing
#   #   register: device_info

#   # ## ne fonctionne pas pour 2 SSD
#   # - name: "[parted] remove all partitions from {{ device }}"
#   #   parted:
#   #     device: "{{ device }}"
#   #     number: "{{ item.num }}"
#   #     state: absent
#   #   loop: "{{ device_info.partitions }}"

#   - name: lvm | lvg > create lvm group on {{ 'luks container' if _luks else (disk.device + _nvme_p + _part_sys_num) }}
#     lvg:
#       vg:  "{{ disk.vg | default('lvm') }}"
#       pvs: "{{ ('/dev/mapper/' + disk.luks.name | default('luks')) if _luks else ('/dev/' + disk.device + _nvme_p + _part_sys_num) }}"
#       # v2 pvs: "/dev/mapper/{{ luks.name | default('luks') }}"
#       # v1 pvs: "/dev/mapper/{{ luks.name }}1,/dev/mapper/{{ luks.name }}2"

#   ## déplacer dans roles/filesystem
#   # - name: "[file] mkdir {{ disk[this].chroot }}/{usr,var,home,boot}"
#   #   file:
#   #     path: "{{ disk[this].chroot }}/{{ item.path }}"
#   #     state: "{{ item.state }}"
#   #   with_items:
#   #     # - { path: "{{ ls.stdout_lines }}", state: absent }  # delete files on {{ arch_root }}"          # j'ai supprimer par erreur la tasks avec register: ls
#   #     - { path: usr,  state: directory }
#   #     - { path: var,  state: directory }
#   #     - { path: home, state: directory }
#   #     - { path: boot, state: directory }

#   - name: lvm | shell > get RAM size for SWAP size
#     shell: grep -i memtotal /proc/meminfo | grep -o "[0-9]*"
#     register: _ram_size

#   # - name: "[lvol] create logical volume root,var,usr,swap,home"
#   #   lvol:
#   #     vg:   "{{ disk.vg | default('lvm') }}"
#   #     lv:   "{{ item.volume }}"
#   #     size: "{{ item.size }}"
#   #     pvs:  "{{ ('/dev/mapper/' + disk.luks.name | default('luks')) if disk.luks.enable else (disk.device + _part_sys_num) }}"
#   #   loop:
#   #     - { volume: root,  size: "{{ _part_sys.root }}g"    , fs: ext4  }
#   #     - { volume: var,   size: "{{ _part_sys.var }}g"     , fs: ext4  }
#   #     - { volume: usr,   size: "{{ _part_sys.usr }}g"     , fs: ext4  }
#   #     - { volume: swap,  size: "{{ (_part_sys.swap + 'g') if _part_sys.swap is defined else ((_ram_size.stdout | int) * 2 + 'k') }}", fs: "{{ disk.partition.system.fs }}" }      ## swap doit être = RAM x 2 et si possible split sur un maximum de disk pour augmenter les perfs
#   #     - { volume: home,  size: "{{ _part_system.home }}%FREE", fs: ext4  }
#   #   when: disk.partition.system.fs == "ext4"

#   - name: lvm | lvol > create logical volume swap and btrfs
#     lvol:
#       vg:   "{{ disk.vg | default('lvm') }}"
#       lv:   "{{ item.volume }}"
#       size: "{{ item.size }}"
#       pvs:  "{{ ('/dev/mapper/' + disk.luks.name | default('luks')) if _luks else ('/dev/' + disk.device + _nvme_p + _part_sys_num) }}"
#     loop:
#       - { volume: swap,  size: "{{ (_part_sys.swap + 'g') if _part_sys.swap is defined else (((_ram_size.stdout | int) * 2) | string + 'k') }}" }
#       - { volume: btrfs, size: "100%FREE", fs: btrfs }
#     when: disk.fs == "btrfs"
